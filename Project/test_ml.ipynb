{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bit6bbedd458cab4a15b0104a5bef4e174f",
   "display_name": "Python 3.8.3 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "9cc99a4f2b96506582f0cda5a47e2d6ebd74c034f63b5627ddd354f4018e771c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = '../data'\n",
    "try: \n",
    "    os.mkdir(path) \n",
    "except OSError as error: \n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "target_path = '../data/aclImdb_v1.tar.gz'\n",
    "\n",
    "response = requests.get(url, stream=True)\n",
    "if response.status_code == 200:\n",
    "    with open(target_path, 'wb') as f:\n",
    "        f.write(response.raw.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile, zipfile\n",
    "with tarfile.open('../data/aclImdb_v1.tar.gz') as tar:\n",
    "    tar.extractall(path='../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'train': {'pos': []}} -------- {'train': {'pos': []}}\n../data/aclImdb\\train\\pos\\*.txt\n{'train': {'pos': [], 'neg': []}} -------- {'train': {'pos': [], 'neg': []}}\n../data/aclImdb\\train\\neg\\*.txt\n{'train': {'pos': [], 'neg': []}, 'test': {'pos': []}} -------- {'train': {'pos': [], 'neg': []}, 'test': {'pos': []}}\n../data/aclImdb\\test\\pos\\*.txt\n{'train': {'pos': [], 'neg': []}, 'test': {'pos': [], 'neg': []}} -------- {'train': {'pos': [], 'neg': []}, 'test': {'pos': [], 'neg': []}}\n../data/aclImdb\\test\\neg\\*.txt\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "data = {}\n",
    "labels = {}\n",
    "data_dir='../data/aclImdb'\n",
    "\n",
    "for data_type in ['train', 'test']:\n",
    "    data[data_type] = {}\n",
    "    labels[data_type] = {}\n",
    "    \n",
    "\n",
    "    for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            print(data, '--------', labels)\n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            print(path)\n",
    "            files = glob.glob(path)\n",
    "            #print(files,'\\n')\n",
    "            \n",
    "            for f in files:\n",
    "                with open(f, encoding=\"utf8\") as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "                    \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "\n",
    "            \n",
    "#print(data, '\\n', labels0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_imdb_data(data, labels):\n",
    "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
    "    \n",
    "    #Combine positive and negative reviews and labels\n",
    "    data_train = data['train']['pos'] + data['train']['neg']\n",
    "    data_test = data['test']['pos'] + data['test']['neg']\n",
    "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
    "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
    "    \n",
    "    #Shuffle reviews and corresponding labels within training and test sets\n",
    "    data_train, labels_train = shuffle(data_train, labels_train)\n",
    "    data_test, labels_test = shuffle(data_test, labels_test)\n",
    "    \n",
    "    # Return a unified training data, test data, training labels, test labets\n",
    "    return data_train, data_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "IMDb reviews (combined): train = 25000, test = 25000\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"A low budget effort from Texas that's at least filmed well, but that is little consolation. Bad acting, or, should I say, bad over-acting, a pretty limp story line that's nothing new, bad special effects, bad, bad, bad. Seems like a bunch of young folks are putting together a haunted house for Halloween, which is done every year, but this year things are different. Has a long extended lesbian theme that is not only annoying but definitely fills out the empty spots, of which there are a lot. Putrid, puerile, definitely avoidable, at all costs.\""
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "train_X[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\", quiet=True)\n",
    "stemmer = PorterStemmer()\n",
    "text = BeautifulSoup(train_X[100], \"html.parser\").get_text()\n",
    "text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "words = text.split()\n",
    "words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "print(len(words))\n",
    "words =[PorterStemmer().stem(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}